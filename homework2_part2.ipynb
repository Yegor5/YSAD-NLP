{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Salary prediction, episode II: make it actually work (4 points)\n",
    "\n",
    "Your main task is to use some of the tricks you've learned on the network and analyze if you can improve __validation MAE__. Try __at least 3 options__ from the list below for a passing grade. Write a short report about what you have tried. More ideas = more bonus points. \n",
    "\n",
    "__Please be serious:__ \" plot learning curves in MAE/epoch, compare models based on optimal performance, test one change at a time. You know the drill :)\n",
    "\n",
    "You can use either __pytorch__ or __tensorflow__ or any other framework (e.g. pure __keras__). Feel free to adapt the seminar code for your needs. For tensorflow version, consider `seminar_tf2.ipynb` as a starting point.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>should_ban</th>\n",
       "      <th>comment_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>The picture on the article is not of the actor...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Its madness. Shes of Chinese heritage, but JAP...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>Fuck You. Why don't you suck a turd out of my ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>God is dead\\nI don't mean to startle anyone bu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>THIS USER IS A PLANT FROM BRUCE PERENS AND GRO...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>995</th>\n",
       "      <td>0</td>\n",
       "      <td>rowspan=9 colspan=8|Did Not Qualify</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>996</th>\n",
       "      <td>0</td>\n",
       "      <td>\"== Disputed and under-referenced ==\\n\\nI have...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>997</th>\n",
       "      <td>0</td>\n",
       "      <td>Why?\\nWhy does this event have its own page? 1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>998</th>\n",
       "      <td>0</td>\n",
       "      <td>Que? \\n\\nWas this fat fingers? If not,  can yo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999</th>\n",
       "      <td>1</td>\n",
       "      <td>so everytime i reset my modem my ip changes\\n\\...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1000 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     should_ban                                       comment_text\n",
       "0             0  The picture on the article is not of the actor...\n",
       "1             1  Its madness. Shes of Chinese heritage, but JAP...\n",
       "2             1  Fuck You. Why don't you suck a turd out of my ...\n",
       "3             1  God is dead\\nI don't mean to startle anyone bu...\n",
       "4             1  THIS USER IS A PLANT FROM BRUCE PERENS AND GRO...\n",
       "..          ...                                                ...\n",
       "995           0                rowspan=9 colspan=8|Did Not Qualify\n",
       "996           0  \"== Disputed and under-referenced ==\\n\\nI have...\n",
       "997           0  Why?\\nWhy does this event have its own page? 1...\n",
       "998           0  Que? \\n\\nWas this fat fingers? If not,  can yo...\n",
       "999           1  so everytime i reset my modem my ip changes\\n\\...\n",
       "\n",
       "[1000 rows x 2 columns]"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "data = pd.read_csv(\"comments.tsv\", sep='\\t')\n",
    "texts = data['comment_text'].values\n",
    "target = data['should_ban'].values\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import TweetTokenizer\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "\n",
    "tokenizer = TweetTokenizer()\n",
    "sentences = []\n",
    "for i in range(len(texts)):\n",
    "    sentences.append(tokenizer.tokenize(texts[i].lower()))\n",
    "model = Word2Vec(sentences=sentences, min_count=1).wv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_len = 0\n",
    "for s in sentences:\n",
    "    max_len = max(max_len, len(s))\n",
    "for s in sentences:\n",
    "    while len(s) < max_len:\n",
    "        s.append('.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = torch.empty((1000, 100, 1247))\n",
    "index = 0\n",
    "for s in sentences:\n",
    "    for i in range(len(s)):\n",
    "        word = torch.tensor(model[s[i]])\n",
    "        if i == 0:\n",
    "            vector_s = word.view(100, 1)\n",
    "        else:\n",
    "            vector_s = torch.cat((vector_s, word.view(100, 1)), 1)\n",
    "    data[index] = vector_s\n",
    "    index += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_target = torch.empty((1000, 2))\n",
    "new_target[:, 0] = torch.tensor(target == 1, dtype=int)\n",
    "new_target[:, 1] = torch.tensor(target == 0, dtype=int)\n",
    "target = new_target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(data, target, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Data(torch.utils.data.Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "        self.len = self.X.shape[0]\n",
    "       \n",
    "    def __getitem__(self, index):\n",
    "        return self.X[index], self.y[index]\n",
    "   \n",
    "    def __len__(self):\n",
    "        return self.len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 50\n",
    "train_dataloader = torch.utils.data.DataLoader(dataset=Data(data, target), batch_size=batch_size, shuffle=True)\n",
    "test_dataloader = torch.utils.data.DataLoader(dataset=Data(data, target), batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "class mynn(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(mynn, self).__init__()\n",
    "        self.conv = nn.Conv1d(100, 10, 3)\n",
    "        self.lin1 = nn.Linear(10, 4)\n",
    "        self.lin2 = nn.Linear(4, 2)\n",
    "        \n",
    "        \n",
    "    def forward(self, X):\n",
    "        X = F.max_pool1d(F.relu(self.conv(X)), 1245)\n",
    "        X = X.view(X.size(0), -1)\n",
    "        X = F.relu(self.lin1(X))\n",
    "        X = self.lin2(X)\n",
    "        return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = mynn()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(100):\n",
    "    for X, y in train_dataloader:\n",
    "        optimizer.zero_grad()\n",
    "        pred = model(X.to(torch.float32))\n",
    "        loss = criterion(pred, y.to(float))\n",
    "        loss.backward()\n",
    "        optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.756\n"
     ]
    }
   ],
   "source": [
    "begin = 0\n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for X, y in test_dataloader:\n",
    "        outputs = model(X.to(torch.float32))\n",
    "        _, predictions = torch.max(outputs, 1)\n",
    "        _, y_true = torch.where(y == 1)\n",
    "        correct += int((y_true == predictions).sum())\n",
    "        total += y_true.shape[0]\n",
    "        end = begin + batch_size\n",
    "        begin = end\n",
    "\n",
    "\n",
    "print(correct / total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "metadata": {},
   "outputs": [],
   "source": [
    "class mynn(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(mynn, self).__init__()\n",
    "        self.conv = nn.ModuleList([nn.Conv1d(100, 10, kernel) for kernel in [2, 3, 4, 5]])\n",
    "        self.lin1 = nn.Linear(40, 10)\n",
    "        self.lin2 = nn.Linear(10, 2)\n",
    "        \n",
    "        \n",
    "    def forward(self, X):\n",
    "        X_conv = [F.relu(conv1d(X)) for conv1d in self.conv]\n",
    "        X_pool = [F.max_pool1d(x_conv, 1248 - i - 2) for i, x_conv in enumerate(X_conv)]\n",
    "        X = torch.cat([x_pool.squeeze(dim=2) for x_pool in X_pool], dim=1)\n",
    "        X = F.relu(self.lin1(X))\n",
    "        X = self.lin2(X)\n",
    "        return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = mynn()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(100):\n",
    "    for X, y in train_dataloader:\n",
    "        optimizer.zero_grad()\n",
    "        pred = model(X.to(torch.float32))\n",
    "        loss = criterion(pred, y.to(float))\n",
    "        loss.backward()\n",
    "        optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.777\n"
     ]
    }
   ],
   "source": [
    "begin = 0\n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for X, y in test_dataloader:\n",
    "        outputs = model(X.to(torch.float32))\n",
    "        _, predictions = torch.max(outputs, 1)\n",
    "        _, y_true = torch.where(y == 1)\n",
    "        correct += int((y_true == predictions).sum())\n",
    "        total += y_true.shape[0]\n",
    "        end = begin + batch_size\n",
    "        begin = end\n",
    "\n",
    "\n",
    "print(correct / total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {},
   "outputs": [],
   "source": [
    "class mynn(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(mynn, self).__init__()\n",
    "        self.conv = nn.ModuleList([nn.Conv1d(100, 10, kernel) for kernel in [2, 3, 4, 5]])\n",
    "        self.lin1 = nn.Linear(40, 10)\n",
    "        self.lin2 = nn.Linear(10, 2)\n",
    "        self.norm = nn.BatchNorm1d(40)\n",
    "        \n",
    "        \n",
    "    def forward(self, X):\n",
    "        X_conv = [F.relu(conv1d(X)) for conv1d in self.conv]\n",
    "        X_pool = [F.max_pool1d(x_conv, 1248 - i - 2) for i, x_conv in enumerate(X_conv)]\n",
    "        X = torch.cat([x_pool.squeeze(dim=2) for x_pool in X_pool], dim=1)\n",
    "        X = self.norm(X)\n",
    "        X = F.relu(self.lin1(X))\n",
    "        X = self.lin2(X)\n",
    "        return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.814\n"
     ]
    }
   ],
   "source": [
    "model = mynn()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.1)\n",
    "for epoch in range(100):\n",
    "    for X, y in train_dataloader:\n",
    "        optimizer.zero_grad()\n",
    "        pred = model(X.to(torch.float32))\n",
    "        loss = criterion(pred, y.to(float))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "begin = 0\n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for X, y in test_dataloader:\n",
    "        outputs = model(X.to(torch.float32))\n",
    "        _, predictions = torch.max(outputs, 1)\n",
    "        _, y_true = torch.where(y == 1)\n",
    "        correct += int((y_true == predictions).sum())\n",
    "        total += y_true.shape[0]\n",
    "        end = begin + batch_size\n",
    "        begin = end\n",
    "\n",
    "\n",
    "print(correct / total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 347,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 50\n",
    "train_dataloader = torch.utils.data.DataLoader(dataset=Data(data, target), batch_size=batch_size, shuffle=True)\n",
    "test_dataloader = torch.utils.data.DataLoader(dataset=Data(data, target), batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "metadata": {},
   "outputs": [],
   "source": [
    "class nn_batch(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(nn_batch, self).__init__()\n",
    "        self.conv = nn.ModuleList([nn.Conv1d(100, 10, kernel) for kernel in [3, 4, 5]])\n",
    "        self.lin1 = nn.Linear(30, 10)\n",
    "        self.lin2 = nn.Linear(10, 2)\n",
    "        self.norm = nn.BatchNorm1d(30)\n",
    "        \n",
    "        \n",
    "    def forward(self, X):\n",
    "        X_conv = [F.relu(conv1d(X)) for conv1d in self.conv]\n",
    "        X_pool = [F.max_pool1d(x_conv, 1248 - i - 3) for i, x_conv in enumerate(X_conv)]\n",
    "        X = torch.cat([x_pool.squeeze(dim=2) for x_pool in X_pool], dim=1)\n",
    "        X = self.norm(X)\n",
    "        X = F.relu(self.lin1(X))\n",
    "        X = self.lin2(X)\n",
    "        return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.872\n"
     ]
    }
   ],
   "source": [
    "model = nn_batch()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.1)\n",
    "for epoch in range(100):\n",
    "    for X, y in train_dataloader:\n",
    "        optimizer.zero_grad()\n",
    "        pred = model(X.to(torch.float32))\n",
    "        loss = criterion(pred, y.to(float))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "begin = 0\n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for X, y in test_dataloader:\n",
    "        outputs = model(X.to(torch.float32))\n",
    "        _, predictions = torch.max(outputs, 1)\n",
    "        _, y_true = torch.where(y == 1)\n",
    "        correct += int((y_true == predictions).sum())\n",
    "        total += y_true.shape[0]\n",
    "        end = begin + batch_size\n",
    "        begin = end\n",
    "\n",
    "\n",
    "print(correct / total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "metadata": {},
   "outputs": [],
   "source": [
    "class nn_batch(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(nn_batch, self).__init__()\n",
    "        self.conv = nn.ModuleList([nn.Conv1d(100, 10, kernel) for kernel in [3, 4, 5]])\n",
    "        self.lin1 = nn.Linear(30, 10)\n",
    "        self.lin2 = nn.Linear(10, 4)\n",
    "        self.lin3 = nn.Linear(4, 2)\n",
    "        self.norm1 = nn.BatchNorm1d(30)\n",
    "        self.norm2 = nn.BatchNorm1d(10)\n",
    "        self.norm3 = nn.BatchNorm1d(4)\n",
    "        \n",
    "        \n",
    "    def forward(self, X):\n",
    "        X_conv = [F.relu(conv1d(X)) for conv1d in self.conv]\n",
    "        X_pool = [F.max_pool1d(x_conv, 1248 - i - 3) for i, x_conv in enumerate(X_conv)]\n",
    "        X = torch.cat([x_pool.squeeze(dim=2) for x_pool in X_pool], dim=1)\n",
    "        X = self.norm1(X)\n",
    "        X = F.relu(self.lin1(X))\n",
    "        X = self.norm2(X)\n",
    "        X = F.relu(self.lin2(X))\n",
    "        X = self.norm3(X)\n",
    "        X = self.lin3(X)\n",
    "        return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.915\n"
     ]
    }
   ],
   "source": [
    "model = nn_batch()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.1)\n",
    "for epoch in range(100):\n",
    "    for X, y in train_dataloader:\n",
    "        optimizer.zero_grad()\n",
    "        pred = model(X.to(torch.float32))\n",
    "        loss = criterion(pred, y.to(float))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "begin = 0\n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for X, y in test_dataloader:\n",
    "        outputs = model(X.to(torch.float32))\n",
    "        _, predictions = torch.max(outputs, 1)\n",
    "        _, y_true = torch.where(y == 1)\n",
    "        correct += int((y_true == predictions).sum())\n",
    "        total += y_true.shape[0]\n",
    "        end = begin + batch_size\n",
    "        begin = end\n",
    "\n",
    "\n",
    "print(correct / total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "metadata": {},
   "outputs": [],
   "source": [
    "class nn_batch(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(nn_batch, self).__init__()\n",
    "        self.conv = nn.ModuleList([nn.Conv1d(100, 10, kernel) for kernel in [3, 4, 5]])\n",
    "        self.lin1 = nn.Linear(30, 10)\n",
    "        self.lin2 = nn.Linear(10, 4)\n",
    "        self.lin3 = nn.Linear(4, 2)\n",
    "        self.norm1 = nn.BatchNorm1d(30)\n",
    "        self.norm2 = nn.BatchNorm1d(10)\n",
    "        self.norm3 = nn.BatchNorm1d(4)\n",
    "        \n",
    "        \n",
    "    def forward(self, X):\n",
    "        X_conv = [F.relu(conv1d(X)) for conv1d in self.conv]\n",
    "        X_pool = [F.avg_pool1d(x_conv, 1248 - i - 3) for i, x_conv in enumerate(X_conv)]\n",
    "        X = torch.cat([x_pool.squeeze(dim=2) for x_pool in X_pool], dim=1)\n",
    "        X = self.norm1(X)\n",
    "        X = F.relu(self.lin1(X))\n",
    "        X = self.norm2(X)\n",
    "        X = F.relu(self.lin2(X))\n",
    "        X = self.norm3(X)\n",
    "        X = self.lin3(X)\n",
    "        return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.715\n"
     ]
    }
   ],
   "source": [
    "model = nn_batch()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.1)\n",
    "for epoch in range(100):\n",
    "    for X, y in train_dataloader:\n",
    "        optimizer.zero_grad()\n",
    "        pred = model(X.to(torch.float32))\n",
    "        loss = criterion(pred, y.to(float))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "begin = 0\n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for X, y in test_dataloader:\n",
    "        outputs = model(X.to(torch.float32))\n",
    "        _, predictions = torch.max(outputs, 1)\n",
    "        _, y_true = torch.where(y == 1)\n",
    "        correct += int((y_true == predictions).sum())\n",
    "        total += y_true.shape[0]\n",
    "        end = begin + batch_size\n",
    "        begin = end\n",
    "\n",
    "\n",
    "print(correct / total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 341,
   "metadata": {},
   "outputs": [],
   "source": [
    "class nn_batch(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(nn_batch, self).__init__()\n",
    "        self.conv = nn.ModuleList([nn.Conv1d(100, 10, kernel) for kernel in [3, 4, 5, 6]])\n",
    "        self.lin1 = nn.Linear(40, 10)\n",
    "        self.lin2 = nn.Linear(10, 4)\n",
    "        self.lin3 = nn.Linear(4, 2)\n",
    "        self.norm1 = nn.BatchNorm1d(40)\n",
    "        self.norm2 = nn.BatchNorm1d(10)\n",
    "        self.norm3 = nn.BatchNorm1d(4)\n",
    "        \n",
    "        \n",
    "    def forward(self, X):\n",
    "        X_conv = [F.relu(conv1d(X)) for conv1d in self.conv]\n",
    "        X_pool = [F.max_pool1d(x_conv, 1248 - i - 3) for i, x_conv in enumerate(X_conv)]\n",
    "        X = torch.cat([x_pool.squeeze(dim=2) for x_pool in X_pool], dim=1)\n",
    "        X = self.norm1(X)\n",
    "        X = F.relu(self.lin1(X))\n",
    "        X = self.norm2(X)\n",
    "        X = F.relu(self.lin2(X))\n",
    "        X = self.norm3(X)\n",
    "        X = self.lin3(X)\n",
    "        return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 342,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.941\n"
     ]
    }
   ],
   "source": [
    "model = nn_batch()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.1)\n",
    "for epoch in range(100):\n",
    "    for X, y in train_dataloader:\n",
    "        optimizer.zero_grad()\n",
    "        pred = model(X.to(torch.float32))\n",
    "        loss = criterion(pred, y.to(float))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "begin = 0\n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for X, y in test_dataloader:\n",
    "        outputs = model(X.to(torch.float32))\n",
    "        _, predictions = torch.max(outputs, 1)\n",
    "        _, y_true = torch.where(y == 1)\n",
    "        correct += int((y_true == predictions).sum())\n",
    "        total += y_true.shape[0]\n",
    "        end = begin + batch_size\n",
    "        begin = end\n",
    "\n",
    "\n",
    "print(correct / total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 348,
   "metadata": {},
   "outputs": [],
   "source": [
    "class nn_batch(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(nn_batch, self).__init__()\n",
    "        self.conv = nn.ModuleList([nn.Conv1d(100, 10, kernel) for kernel in [4, 5, 6, 7]])\n",
    "        self.lin1 = nn.Linear(40, 10)\n",
    "        self.lin2 = nn.Linear(10, 4)\n",
    "        self.lin3 = nn.Linear(4, 2)\n",
    "        self.norm1 = nn.BatchNorm1d(40)\n",
    "        self.norm2 = nn.BatchNorm1d(10)\n",
    "        self.norm3 = nn.BatchNorm1d(4)\n",
    "        \n",
    "        \n",
    "    def forward(self, X):\n",
    "        X_conv = [F.relu(conv1d(X)) for conv1d in self.conv]\n",
    "        X_pool = [F.max_pool1d(x_conv, 1248 - i - 4) for i, x_conv in enumerate(X_conv)]\n",
    "        X = torch.cat([x_pool.squeeze(dim=2) for x_pool in X_pool], dim=1)\n",
    "        X = self.norm1(X)\n",
    "        X = F.relu(self.lin1(X))\n",
    "        X = self.norm2(X)\n",
    "        X = F.relu(self.lin2(X))\n",
    "        X = self.norm3(X)\n",
    "        X = self.lin3(X)\n",
    "        return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 349,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.952\n"
     ]
    }
   ],
   "source": [
    "model = nn_batch()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.1)\n",
    "for epoch in range(100):\n",
    "    for X, y in train_dataloader:\n",
    "        optimizer.zero_grad()\n",
    "        pred = model(X.to(torch.float32))\n",
    "        loss = criterion(pred, y.to(float))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "begin = 0\n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for X, y in test_dataloader:\n",
    "        outputs = model(X.to(torch.float32))\n",
    "        _, predictions = torch.max(outputs, 1)\n",
    "        _, y_true = torch.where(y == 1)\n",
    "        correct += int((y_true == predictions).sum())\n",
    "        total += y_true.shape[0]\n",
    "        end = begin + batch_size\n",
    "        begin = end\n",
    "\n",
    "\n",
    "print(correct / total)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Steps:\n",
    "* One CNN with 10 out_channels, accuracy = 0.756\n",
    "* Four CNNs with different kernels, accuracy = 0.777\n",
    "* Add more linears and batchnorms, accuracy = 0.915\n",
    "* Add more kernels, accuracy = 0.952"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recommended options\n",
    "\n",
    "#### A) CNN architecture\n",
    "\n",
    "All the tricks you know about dense and convolutional neural networks apply here as well.\n",
    "* Dropout. Nuff said.\n",
    "* Batch Norm. This time it's `nn.BatchNorm*`/`L.BatchNormalization`\n",
    "* Parallel convolution layers. The idea is that you apply several nn.Conv1d to the same embeddings and concatenate output channels.\n",
    "* More layers, more neurons, ya know...\n",
    "\n",
    "\n",
    "#### B) Play with pooling\n",
    "\n",
    "There's more than one way to perform pooling:\n",
    "* Max over time (independently for each feature)\n",
    "* Average over time (excluding PAD)\n",
    "* Softmax-pooling:\n",
    "$$ out_{i, t} = \\sum_t {h_{i,t} \\cdot {{e ^ {h_{i, t}}} \\over \\sum_\\tau e ^ {h_{j, \\tau}} } }$$\n",
    "\n",
    "* Attentive pooling\n",
    "$$ out_{i, t} = \\sum_t {h_{i,t} \\cdot Attn(h_t)}$$\n",
    "\n",
    ", where $$ Attn(h_t) = {{e ^ {NN_{attn}(h_t)}} \\over \\sum_\\tau e ^ {NN_{attn}(h_\\tau)}}  $$\n",
    "and $NN_{attn}$ is a dense layer.\n",
    "\n",
    "The optimal score is usually achieved by concatenating several different poolings, including several attentive pooling with different $NN_{attn}$ (aka multi-headed attention).\n",
    "\n",
    "The catch is that keras layers do not inlude those toys. You will have to [write your own keras layer](https://keras.io/layers/writing-your-own-keras-layers/). Or use pure tensorflow, it might even be easier :)\n",
    "\n",
    "#### C) Fun with words\n",
    "\n",
    "It's not always a good idea to train embeddings from scratch. Here's a few tricks:\n",
    "\n",
    "* Use a pre-trained embeddings from `gensim.downloader.load`. See last lecture.\n",
    "* Start with pre-trained embeddings, then fine-tune them with gradient descent. You may or may not download pre-trained embeddings from [here](http://nlp.stanford.edu/data/glove.6B.zip) and follow this [manual](https://keras.io/examples/nlp/pretrained_word_embeddings/) to initialize your Keras embedding layer with downloaded weights.\n",
    "* Use the same embedding matrix in title and desc vectorizer\n",
    "\n",
    "\n",
    "#### D) Going recurrent\n",
    "\n",
    "We've already learned that recurrent networks can do cool stuff in sequence modelling. Turns out, they're not useless for classification as well. With some tricks of course..\n",
    "\n",
    "* Like convolutional layers, LSTM should be pooled into a fixed-size vector with some of the poolings.\n",
    "* Since you know all the text in advance, use bidirectional RNN\n",
    "  * Run one LSTM from left to right\n",
    "  * Run another in parallel from right to left \n",
    "  * Concatenate their output sequences along unit axis (dim=-1)\n",
    "\n",
    "* It might be good idea to mix convolutions and recurrent layers differently for title and description\n",
    "\n",
    "\n",
    "#### E) Optimizing seriously\n",
    "\n",
    "* You don't necessarily need 100 epochs. Use early stopping. If you've never done this before, take a look at [early stopping callback(keras)](https://keras.io/callbacks/#earlystopping) or in [pytorch(lightning)](https://pytorch-lightning.readthedocs.io/en/latest/common/early_stopping.html).\n",
    "  * In short, train until you notice that validation\n",
    "  * Maintain the best-on-validation snapshot via `model.save(file_name)`\n",
    "  * Plotting learning curves is usually a good idea\n",
    "  \n",
    "Good luck! And may the force be with you!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
